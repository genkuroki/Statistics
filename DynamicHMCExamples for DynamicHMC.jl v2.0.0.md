---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.1'
      jupytext_version: 1.2.1
  kernelspec:
    display_name: Julia 1.2.0
    language: julia
    name: julia-1.2
---

<!-- #region -->
# DynamicHMCExamples for DynamicHMC.jl v2.0.0

ÈªíÊú®ÁéÑ

2019-08-26ÔΩû2019-09-03

2019Âπ¥9Êúà3Êó•„Å´

```
pkg> add LogDensityProblems
pkg> add DynamicHMC
```

„Åó„ÅüÁä∂ÊÖã„Åß

https://github.com/tpapp/DynamicHMCExamples.jl/tree/master/src

„Å´„ÅÇ„Çãexamples„ÇíÂãï„Åè„Çà„ÅÜ„Å´„Åó„Åü„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ.

DynamicHMC.jl „ÅØ v2.0.0 „Åã„Çâ‰ªïÊßò„ÅåÂ§ßÂπÖ„Å´Â§âÂåñ„Åô„Çã„ÅÆ„Åß, ÁèæÂú®ÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çãexamples„Åå„Åù„ÅÆ„Åæ„Åæ„Åß„ÅØÂãï„Åã„Å™„ÅÑ.  „Åù„Çå„Çí‰øÆÊ≠£„Åó„Å¶„Åø„Åü„ÅÆ„Åå, „Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„Åô.
<!-- #endregion -->

```julia
]status LogDensityProblems
```

```julia
]status DynamicHMC
```

<!-- #region {"toc": true} -->
<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Bernoulli" data-toc-modified-id="Bernoulli-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Bernoulli</a></span></li><li><span><a href="#Logistic-regression" data-toc-modified-id="Logistic-regression-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href="#Linear-regression" data-toc-modified-id="Linear-regression-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Linear regression</a></span></li></ul></div>
<!-- #endregion -->

```julia
using Distributions
using TransformVariables
using LogDensityProblems
using DynamicHMC
using DynamicHMC.Diagnostics
using MCMCDiagnostics
using Parameters
using Statistics
using Random
using Plots
gr(size=(400, 250), titlefontsize=12)
using ForwardDiff
using StatsFuns
using LinearAlgebra
```

## Bernoulli

https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_independent_bernoulli.jl

```julia
# # Estimate Bernoulli draws probabilility

# We estimate a simple model of ``n`` independent Bernoulli draws, with
# probability ``Œ±``. First, we load the packages we use.

# using TransformVariables
# using LogDensityProblems
# using DynamicHMC
# using MCMCDiagnostics
# using Parameters
# using Statistics

# Then define a structure to hold the data.
# For this model, the number of draws equal to `1` is a sufficient statistic.

"""
Toy problem using a Bernoulli distribution.
We model `n` independent draws from a ``Bernoulli(Œ±)`` distribution.
"""
struct BernoulliProblem
    "Total number of draws in the data."
    n::Int
    "Number of draws `==1` in the data"
    s::Int
end

# Then make the type callable with the parameters *as a single argument*.  We
# use decomposition in the arguments, but it could be done inside the function,
# too.

function (problem::BernoulliProblem)((Œ±, )::NamedTuple{(:Œ±, )})
    @unpack n, s = problem        # extract the data
    # log likelihood: the constant log(combinations(n, s)) term
    # has been dropped since it is irrelevant to sampling.
    s * log(Œ±) + (n-s) * log(1-Œ±)
end

# We should test this, also, this would be a good place to benchmark and
# optimize more complicated problems.

p = BernoulliProblem(30, 10)
p((Œ± = 0.5, ))
```

```julia
# Recall that we need to
#
# 1. transform from ``‚Ñù`` to the valid parameter domain `(0,1)` for more efficient sampling, and
#
# 2. calculate the derivatives for this transformed mapping.
#
# The helper packages `TransformVariables` and `LogDensityProblems` take care of
# this. We use a flat prior (the default, omitted)


trans = as((Œ± = asùïÄ,))
P = TransformedLogDensity(trans, p)
‚àáP = ADgradient(:ForwardDiff, P);

display(trans)
display(P)
display(‚àáP)
```

```julia
# Finally, we sample from the posterior. `chain` holds the chain (positions and
# diagnostic information), while the second returned value is the tuned sampler
# which would allow continuation of sampling.

L = 1000

#chain, NUTS_tuned = NUTS_init_tune_mcmc(‚àáP, L)
@time results = mcmc_with_warmup(Random.GLOBAL_RNG, ‚àáP, L; reporter = NoProgressReport());

display(results.chain)
```

```julia
# To get the posterior for ``Œ±``, we need to use `get_position` and
# then transform

#posterior = transform.(Ref(t), get_position.(chain));
posterior = transform.(trans, results.chain)
```

```julia
# Extract the parameter.

posterior_Œ± = first.(posterior)
```

```julia
# check the mean

mean(posterior_Œ±)
```

```julia
# check the effective sample size

ess_Œ± = effective_sample_size(posterior_Œ±)
```

```julia
# NUTS-specific statistics

#NUTS_statistics(chain)
summarize_tree_statistics(results.tree_statistics)
```

```julia
P1 = plot(posterior_Œ±)
P2 = histogram(posterior_Œ±)
plot(P1, P2, size=(800, 250), title="alpha", legend=false)
```

## Logistic regression

https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_logistic_regression.jl

```julia
# # Logistic regression

#using TransformVariables, LogDensityProblems, DynamicHMC, MCMCDiagnostics, Parameters,
#    Distributions, Statistics, StatsFuns, ForwardDiff

"""
Logistic regression.
For each draw, ``logit(Pr(y·µ¢ == 1)) ‚àº X·µ¢ Œ≤``. Uses a `Œ≤ ‚àº Normal(0, œÉ)` prior.
`X` is supposed to include the `1`s for the intercept.
"""
struct LogisticRegression{Ty, TX, TœÉ}
    y::Ty
    X::TX
    œÉ::TœÉ
end

function (problem::LogisticRegression)(Œ∏)
    @unpack y, X, œÉ = problem
    @unpack Œ≤ = Œ∏
    loglik = sum(logpdf.(Bernoulli.(logistic.(X*Œ≤)), y))
    logpri = sum(logpdf.(Ref(Normal(0, œÉ)), Œ≤))
    loglik + logpri
end

# Make up parameters, generate data using random draws.

N = 1000
Œ≤ = [1.0, 2.0]
X = hcat(ones(N), randn(N))
y = rand.(Bernoulli.(logistic.(X*Œ≤)));
```

```julia
# Create a problem, apply a transformation, then use automatic differentiation.

p = LogisticRegression(y, X, 10.0)   # data and (vague) priors
```

```julia
trans = as((Œ≤ = as(Array, length(Œ≤)), )) # identity transformation, just to get the dimension
P = TransformedLogDensity(trans, p)      # transformed
‚àáP = ADgradient(:ForwardDiff, P)

display(trans)
display(P)
display(‚àáP)
```

```julia
# Sample using NUTS, random starting point.

#chain, NUTS_tuned = NUTS_init_tune_mcmc(‚àáP, 1000);

L = 10^3
@time results = mcmc_with_warmup(Random.GLOBAL_RNG, ‚àáP, L; reporter = NoProgressReport());
display(results.chain)
```

```julia
# Extract the posterior. Here the transformation was not really necessary.

posterior = transform.(trans, results.chain)
```

```julia
Œ≤_posterior = first.(posterior)
```

```julia
# Check that we recover the parameters.

mean(Œ≤_posterior)
```

```julia
# Quantiles

qs = [0.05, 0.25, 0.5, 0.75, 0.95]
quantile(first.(Œ≤_posterior), qs)
```

```julia
quantile(last.(Œ≤_posterior), qs)
```

```julia
# Check that mixing is good.

ess = vec(mapslices(effective_sample_size, reduce(hcat, Œ≤_posterior); dims = 2))
```

```julia
P1 = plot(first.(Œ≤_posterior))
P2 = histogram(first.(Œ≤_posterior))
plot(P1, P2, size=(800, 250), title="beta1", legend=false)
```

```julia
P1 = plot(last.(Œ≤_posterior))
P2 = histogram(last.(Œ≤_posterior))
plot(P1, P2, size=(800, 250), title="beta2", legend=false)
```

## Linear regression

https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_linear_regression.jl

```julia
Random.seed!(4649);
```

```julia
# # Linear regression

# We estimate simple linear regression model with a half-T prior.
# First, we load the packages we use.

# using TransformVariables, LogDensityProblems, DynamicHMC, MCMCDiagnostics,
#     Parameters, Statistics, Distributions, ForwardDiff

# Then define a structure to hold the data: observables, covariates, and the degrees of freedom for the prior.

"""
Linear regression model ``y ‚àº XŒ≤ + œµ``, where ``œµ ‚àº N(0, œÉ¬≤)`` IID.
Flat prior for `Œ≤`, half-T for `œÉ`.
"""
struct LinearRegressionProblem{TY <: AbstractVector, TX <: AbstractMatrix,
                               TŒΩ <: Real}
    "Observations."
    y::TY
    "Covariates"
    X::TX
    "Degrees of freedom for prior."
    ŒΩ::TŒΩ
end

# Then make the type callable with the parameters *as a single argument*.

function (problem::LinearRegressionProblem)(Œ∏)
    @unpack y, X, ŒΩ = problem   # extract the data
    @unpack Œ≤, œÉ = Œ∏            # works on the named tuple too
    loglikelihood(Normal(0, œÉ), y .- X*Œ≤) + logpdf(TDist(ŒΩ), œÉ)
end

# We should test this, also, this would be a good place to benchmark and
# optimize more complicated problems.

#N = 100
N = 5
#X = hcat(ones(N), randn(N, 2));
x = rand(Uniform(-2, 2), N)
X = hcat(ones(N), x, x.^2);
Œ≤ = [1.0, 2.0, -1.0]
#œÉ = 0.5
œÉ = 1.0
y = X*Œ≤ .+ randn(N) .* œÉ;
p = LinearRegressionProblem(y, X, 1.0);
p((Œ≤ = Œ≤, œÉ = œÉ))
```

```julia
law_true(Œ≤, x) = Œ≤[1] + Œ≤[2]*x + Œ≤[3]*x^2
```

```julia
# For this problem, we write a function to return the transformation (as it varies with the number of covariates).

problem_transformation(p::LinearRegressionProblem) =
    as((Œ≤ = as(Array, size(p.X, 2)), œÉ = as‚Ñù‚Çä))

# Wrap the problem with a transformation, then use Flux for the gradient.

trans = problem_transformation(p)
P = TransformedLogDensity(trans, p)
‚àáP = ADgradient(:ForwardDiff, P);

display(trans)
display(P)
display(‚àáP)
```

```julia
# Finally, we sample from the posterior. `chain` holds the chain (positions and
# diagnostic information), while the second returned value is the tuned sampler
# which would allow continuation of sampling.

L = 2^11

#chain, NUTS_tuned = NUTS_init_tune_mcmc(‚àáP, 1000);
@time results = mcmc_with_warmup(Random.GLOBAL_RNG, ‚àáP, L; reporter = NoProgressReport());

display(results.chain)
```

```julia
# We use the transformation to obtain the posterior from the chain.

#posterior = transform.(Ref(t), get_position.(chain));
posterior = transform.(trans, results.chain)
```

```julia
# Extract the parameter posterior means: `Œ≤`,

posterior_Œ≤ = first.(posterior)
@show mean_posterior_Œ≤ = mean(posterior_Œ≤)

# then `œÉ`:

posterior_œÉ = last.(posterior)
@show mean_posterior_œÉ = mean(posterior_œÉ)

display(posterior_Œ≤)
display(posterior_œÉ)
```

```julia
# Effective sample sizes (of untransformed draws)

ess = mapslices(effective_sample_size, hcat(results.chain...); dims = 2)
```

```julia
# NUTS-specific statistics

#NUTS_statistics(chain)
summarize_tree_statistics(results.tree_statistics)
```

```julia
scatter(X[:,2], y, label="sample", legend=:bottomright)
plot!(x -> law_true(Œ≤, x), label="true law", ls=:dash)
```

```julia
P1 = plot((x->x[1]).(posterior_Œ≤))
P2 = histogram((x->x[1]).(posterior_Œ≤))
plot(P1, P2, size=(800, 250), title="beta1", legend=false)
```

```julia
P1 = plot((x->x[2]).(posterior_Œ≤))
P2 = histogram((x->x[2]).(posterior_Œ≤))
plot(P1, P2, size=(800, 250), title="beta2", legend=false)
```

```julia
P1 = plot((x->x[3]).(posterior_Œ≤))
P2 = histogram((x->x[3]).(posterior_Œ≤))
plot(P1, P2, size=(800, 250), title="beta3", legend=false)
```

```julia
P1 = plot(posterior_œÉ)
P2 = histogram(posterior_œÉ)
plot(P1, P2, size=(800, 250), title="sigma", legend=false)
```

```julia
function pred_linreg(posterior, x, y)
    L = length(posterior)
    Œ≤ = first.(posterior)
    œÉ = last.(posterior)
    mean(pdf(Normal(0.0, œÉ[i]), y - (Œ≤[i][1] + Œ≤[i][2]*x + Œ≤[i][3]*x^2)) for i in 1:L)
end
```

```julia
@show a = (X'X)\X'*y
@show s = fit_mle(Normal, y - X*a).œÉ
pred_linreg_mle(a, s, x, y) = pdf(Normal(0.0, s), y - (a[1] + a[2]*x + a[3]*x^2))
```

```julia
xs = range(-4, 4, length=100)
ys = range(-23, 7, length=100)

@time zs = [pred_linreg(posterior, x, y) for y in ys, x in xs]
P1 = plot(; size=(400, 300))
heatmap!(xs, ys, zs, colorbar=false)
scatter!(X[:,2], y, label="sample", legend=false, markersize=4, color=:cyan, alpha=0.7)
plot!(xs, [law_true(Œ≤, x) for x in xs], ls=:dash, lw=2)
title!("Bayes")

@time zs = [pred_linreg_mle(a, s, x, y) for y in ys, x in xs]
P2 = plot(; size=(400, 300))
heatmap!(xs, ys, zs, colorbar=false)
scatter!(X[:,2], y, label="sample", legend=false, markersize=4, color=:cyan, alpha=0.7)
plot!(xs, [law_true(Œ≤, x) for x in xs], ls=:dash, lw=2)
title!("MLE")

plot(P1, P2, size=(800, 300))
```

```julia
2N*mean(-log(pred_linreg(posterior, X[i,2], y[i])) for i in 1:N) 
```

```julia
2N*mean(-log(pred_linreg_mle(a, s, X[i,2], y[i])) for i in 1:N) 
```

```julia

```
